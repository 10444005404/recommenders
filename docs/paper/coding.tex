\subsection{Code Design}
\label{code-style}

We strive to maintain high quality code to make the utilities in the repository easy to 
understand, use, and extend. We also work hard to maintain a friendly and constructive 
environment. We've found that having clear expectations on the development process 
and consistent style helps to ensure everyone can contribute and collaborate effectively.

We have published in the repository wiki the coding 
guidelines\footnote{\url{https://github.com/Microsoft/Recommenders/wiki/Coding-Guidelines}\label{foot_code_guidelines}} 
for the project. Next, we describe the most important parts.

\subsubsection{Evidence-Based Software Design}
At its core, Evidence-Based Design (EBD) \cite{kembel2012architectural,joeglekar2018evidence} embodies the 
scientific method: it empowers one to ask the right questions and develop hypotheses, 
gather quantitative and qualitative data that support or disprove these hypotheses, 
and measure, share, and learn from the outcomes.

In the case of Recommenders, the evidence is gathered from extensive experience working 
with customers in real life recommendation scenarios. When there is no input from a 
customer, the second source of evidence that is used is popular machine learning
libraries like Tensorflow \cite{abadi2016tensorflow}, PyTorch \cite{paszke2017automatic},
Scikit-learn \cite{pedregosa2011scikit} or LigthGBM \cite{ke2017lightgbm}.

One example to illustrate EBD is the definition of the metrics interface in Python and 
PySpark. The definition in Python as based of functions:

\begin{minted}{python}
    from reco_utils.evaluation.python_evaluation import rmse
    result = rmse(df_test, df_predictions)
\end{minted}

whereas the definition in PySpark is based of classes:
\begin{minted}{python}
    from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation
    rating_eval = SparkRatingEvaluation(df_test, df_prediction)
    result = rating_eval.rmse()
\end{minted}

The selection of classes in PySpark and functions in Python, instead of having a unified
interface, is derived from the EDB principle. Our customers and machine learning users
that code in Python will naturally choose a function when creating metrics, as it is
defined in Scikit-learn \cite{pedregosa2011scikit}. On the contrary, PySpark users 
will tend to code based on classes as defined in \cite{meng2016mllib}. 

\subsubsection{Test Pipeline}

The test pipeline in Recommenders is slightly more complicated than in most machine learning
libraries \cite{abadi2016tensorflow,paszke2017automatic,pedregosa2011scikit,ke2017lightgbm}.
Apart from standard {\em unit tests} with PyTest \cite{krekel2004pytest} of the utilities, 
the Jupyter notebooks are also tested. To perform these tests we use Papermill 
\cite{nteract2017papermill}, which allows for programmatic execution of a notebook. The
unit tests are executed in every pull request and ensure that the utilities and 
notebooks run without an error.

We have also included nightly tests composed by {\em smoke} and {\em integration tests}
\cite{gonzalez-fierro2018beginners}. In the smoke tests, we run the notebooks with a 
small dataset or a small number of epochs to make sure that, apart from running, they 
provide reasonable metrics. In the following example we show how to make sure that
the precision at k of SAR algorithm with Movielens 100k dataset is tested.

\begin{minted}{python}
    TOL = 0.05
    @pytest.mark.smoke
    def test_sar_single_node_smoke(notebooks):
        notebook_path = notebooks["sar_single_node"]
        pm.execute_notebook(
            notebook_path,
            OUTPUT_NOTEBOOK,
            parameters=dict(TOP_K=10, MOVIELENS_DATA_SIZE="100k"),
        )
        results = pm.read_notebook(OUTPUT_NOTEBOOK).dataframe.set_index("name")["value"]
        assert results["precision"] == pytest.approx(0.326617179, TOL)
\end{minted}

The smoke tests are a small version of the integration tests. While the integration tests
use bigger datasets with more epochs and can take from 30min to several hours, the smoke tests are quicker and
will check the correctness of all the notebooks before executing the integration tests.

At the time of writing we have over 400 tests, between Windows and Linux. Furthermore, we have two group of tests,
one for master branch and one for staging branch, which is our development branch.

The existence of a development branch, staging, is tightly related to the test pipeline. The paradigm is similar to
production and pre-production environments. The production branch is master, while the pre-preproduction branch is staging.
The objective is to make sure that the code in master always work. When making a pull request, we will
do it against staging, instead of master. The nightly tests will execute in staging and make sure that the new code 
works as expected, apart from not failing. When staging is stable, we will make a pull request to master.  


\subsubsection{Other Design Patterns}

We introduced a number of design patters that are standard in the industry and well-known
in the open source community. For instance, \textit{don't repeat yourself}, \textit{single
responsibility} or the Zen of Python\footnote{\url{https://www.python.org/dev/peps/pep-0020/}}. 

The following example illustrates the pattern \textit{explicit is better than implicit}.
An implicit read function would be: 
\begin{minted}{python}
    def read(filename):
        # code for reading a csv or json
        # depending on the file extension
\end{minted}

\begin{minted}{python}
    def read(filename):
        # code for reading a csv or json
        # depending on the file extension
\end{minted}

whereas the explicit example would be:
\begin{minted}{python}
    def read_csv(filename):
        # code for reading a csv

    def read_json(filename):
        # code for reading a json
\end{minted}



