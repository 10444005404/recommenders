\subsection{Code Design}
\label{code-style}

We strive to maintain high quality code so as to make the utilities in the repository easy to 
understand, use, and extend. We also strive to maintain a friendly and constructive 
environment. We have found that having clear expectations on the development process 
and consistent style helps to ensure everyone can contribute and collaborate effectively.
We have published the coding 
guidelines\footnote{\url{https://github.com/Microsoft/Recommenders/wiki/Coding-Guidelines}\label{foot_code_guidelines}} 
in the wiki for the repository. Next, we describe the most important parts.

\subsubsection{Evidence-Based Software Design}
At its core, Evidence-Based Design (EBD) \cite{kembel2012architectural,joeglekar2018evidence} embodies the 
scientific method: it empowers one to ask the right questions and develop hypotheses, 
gather quantitative and qualitative data that support or disprove these hypotheses, 
and measure, share, and learn from the outcomes.

In the case of Recommenders, the evidence is gathered from extensive experience working 
with customers in real life recommendation scenarios. When there is no input from a 
customer, the second source of evidence that is used is popular machine learning
libraries like Tensorflow \cite{abadi2016tensorflow}, PyTorch \cite{paszke2017automatic},
Scikit-learn \cite{pedregosa2011scikit} or LightGBM \cite{ke2017lightgbm}.

One example illustrating EBD is the definition of the metrics interfaces in Python (not in a Spark context) and 
PySpark \cite{meng2016mllib}. The definition in Python is based on functions:

\begin{minted}{python}
    from reco_utils.evaluation.python_evaluation import rmse
    result = rmse(df_test, df_predictions)
\end{minted}

whereas the definition in PySpark is based on classes:
\begin{minted}{python}
    from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation
    rating_eval = SparkRatingEvaluation(df_test, df_prediction)
    result = rating_eval.rmse()
\end{minted}

The selection of classes in PySpark and functions in Python, instead of having a unified
interface, is derived from the EDB principle. Any machine learning practitioners
who code in Python will naturally choose a function when computing metrics, 
in a way similar to Scikit-learn \cite{pedregosa2011scikit}. By contrast, PySpark users 
will tend to code based on classes as defined in \cite{meng2016mllib}. 

\subsubsection{Test Pipeline}

The test pipeline in Recommenders is more complicated than in most machine learning
libraries \cite{abadi2016tensorflow,paszke2017automatic,pedregosa2011scikit,ke2017lightgbm}.
Apart from standard {\em unit tests} of the utilities with PyTest \cite{krekel2004pytest}, 
the Jupyter notebooks are also tested. To perform the latter tests we use Papermill 
\cite{nteract2017papermill}, which allows for programmatic execution of notebooks. The
unit tests are executed with every pull request and ensure that the utilities and 
notebooks run without error.

We have also included nightly tests consisting of {\em smoke} and {\em integration tests}
\cite{gonzalez-fierro2018beginners}. In the smoke tests, we run the notebooks with a 
small dataset or a small number of epochs to ensure that they execute without failure and that they 
output reasonable metrics. In the following example we show how 
the precision at k of SAR algorithm with Movielens 100k dataset is tested.

\begin{minted}{python}
    TOL = 0.05
    @pytest.mark.smoke
    def test_sar_single_node_smoke(notebooks):
        notebook_path = notebooks["sar_single_node"]
        pm.execute_notebook(
            notebook_path,
            OUTPUT_NOTEBOOK,
            parameters=dict(TOP_K=10, MOVIELENS_DATA_SIZE="100k"),
        )
        results = pm.read_notebook(OUTPUT_NOTEBOOK).dataframe.set_index("name")["value"]
        assert results["precision"] == pytest.approx(0.326617179, TOL)
\end{minted}

The smoke tests are a small version of the integration tests. While the integration tests
use bigger datasets with more epochs and can take from 30 minutes to several hours, the smoke tests are quicker and
will check for correctness of all the notebooks before executing the integration tests.
At the time of writing there are over 400 tests, between Windows and Linux. There are two groups of tests,
one for the {\em master branch} and one for the {\em staging branch}, which is our development branch.

The existence of a development branch, staging, is tightly related to the test pipeline. This paradigm is similar to
production and pre-production environments (master and staging, respectively).
The objective is to ensure that the code in master always works. When making a pull request, we 
do it against staging, instead of master. The nightly tests execute in staging and make sure that the new code 
works as expected and does not fail. When staging is stable, we merge it to master with a pull request.  


\subsubsection{Other Design Patterns}

We have introduced a number of design patterns which are standard in industry and well-known
in the open source community. For instance, \textit{don't repeat yourself}, \textit{single
responsibility} or the Zen of Python\footnote{\url{https://www.python.org/dev/peps/pep-0020/}}. 

The following example illustrates the pattern \textit{explicit is better than implicit}.
An implicit read function would be: 
\begin{minted}{python}
    def read(filename):
        # code for reading a csv or json
        # depending on the file extension
\end{minted}

whereas the explicit example would be:
\begin{minted}{python}
    def read_csv(filename):
        # code for reading a csv

    def read_json(filename):
        # code for reading a json
\end{minted}



