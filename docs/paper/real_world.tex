\section{Lessons Learned from Building Real-World Systems} 

The core development team of Microsoft Recommenders have worked with data science and machine learning engineering team in various industrial verticals, helping them develop the production-ready recommender system for different types of problems, like, video recommendation, restaurant recommendation, etc. Development of recommender system usually consists of intensive engineering work for data preprocessing, model building, etc. A recommender system is often used in a scenario where data have big volume, change dramatically, and have high dimensionality, e.g., e-commerce. This requires data scientists and machine learning engineers design scalable and reliable system. 

There are basically two representatives in the contemporary recommendation system architecture design.

\subsection{Batch-mode recommendation}
The batch-mode recommendation refers to a pipeline where a recommender model is built with the input data flow, and the recommendation results from that model are generated successively. In industrial recommender system where a collaborative filtering typed algorithm is used, this kind of architecture is highly effective. This is because the batch-mode pipeline hides the latency in using a model to score items and produce the top-k recommendations, by pre-caching the recommendation results into a database (usually this database is globally distributed for querying efficiency in different regions). 

An end-to-end batch-mode recommendation pipeline usually consists of data preprocessing, model building, and item recommending. Model management may not be critical in such arrangement as model building is an intermediate step that works for generating the results. Observability of the entire pipeline is often important, because this allows the engineer to debug the pipeline without any difficulty. Metric logging tool in the pipeline management is often used \cite{zaharia2018accelerating}. A detailed reference architecture built on top of Microsoft Azure services that demonstrates such batch-mode recommendation, which has been used in the real-world customer use cases, can be found in the Azure official documentation \footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/ai/real-time-recommendation}}

\subsection{Real-time model serving}
In the scenarios where attributes of users and items, contextual information, etc., with high dimensions, are used in model building, batch-mode recommendation architecture may not appropriately apply, and a real-time model serving scheme is usually preferred. Compared to the batch-mode recommendation, model management in this scenario is specially importance because a pre-trained model is re-used for scoring and item ranking. Scalability of computation for model scoring is also vital in this scenario. Depending on business requirements, engineering specifications, and model complexity, computing resources for model serving should be scalable and configurable for both computational efficiency and cost saving purpose. Nowadays, Kubernetes cluster is used as the industrial main stream for model serving \cite{bernstein2014containers}. Another reference architecture that illustrates the use of LightGBM model \cite{ke2017lightgbm} for real-time model serving can also be found in the Azure documentation \footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/scalable-personalization}}


