\section{Lessons Learned from Building Real-World Systems} 

Productionizing a industry-grade recommender system for real-world problems is often challenging in a various aspects. For example, {\em model retraining} on a daily basis is often crucial in applications like e-commerce where the volume and dynamics of training data can change rapidly. 
In some cases, due to the size of the data or effects like concept drift, {\em online training} may be the best option. 
{\em Hyperparameters} of a recommender model, especially in content-based recommendation scenarios, need to be fine tuned properly in order to ensure that the model delivers good performance.
{\em Real-time serving} of recommendations should meet engineering specifications determined from business requirements, 
typically in the order of 100 ms. An end-to-end recommendation pipeline may consist of {\em heterogeneous computing platforms} 
(distributed computing cluster, GPU etc.) where computing resources should be used effectively and economically. 
The {\em health status} of the recommendation system should be well monitored with an online mechanism 
so that break points of the entire pipeline can be detected. 
{\em Offline evaluation} is often used for selection and tuning of algorithms, while it is always recommended to perform {\em online evaluation} 
by using A/B testing and/or multi-armed bandits; a frequent issue however is that offline and online evaluation may not correlate well. 
Finally, a recommender system may be restricted by {\em business rules} (such as country-specific legislation, 
seen items allowed to be recommended only a certain amount of times, seasonal restrictions etc.) 
to make the recommendation results comply.

%\begin{itemize}
%    \item Model training and retraining in time is usually crucial to real-world applications like e-commerce where the volume and dynamics of training data change dramatically. This requires iterative optimization for model implementation as well as architecture construction.
%    \item Hyperparameters of a recommender model, especially that used for content-based recommendation scenarios, needs to be fine tuned properly in order to make sure the model delivers good performance. 
%    \item Real-time model or recommendation results serving should meet the engineering specifications determined from business requirements. 
%    \item An end-to-end recommendation pipeline may consist of various computing framework (e.g., distributed computing cluster, GPU, etc.) where computing resources should be used effectively and economically. 
%    \item Health status of a recommender system should be well managed, and online monitoring mechanism is therefore needed such that break points of the entire pipeline is observable.
%    \item Offline evaluation is often used for selection of algorithm, in comparison to baseline models, while it is always recommended to perform online evaluation by using A/B testing and/or multi-armed bandit for fairly evaluating a recommender system.
%    \item A recommender system is restricted by business rules like country-specific legislation (e.g., seen items are allowed to be recommended by only a certain amount of times, if it is in summer a jumper should not be recommended, etc) to make the recommendation results comply.
%\end{itemize}

%\subsection{Reference Architectures}
%
%To effectively mitigate most (if not all) of the aforementioned challenges, 
Microsoft Recommenders provides two end-to-end reference architecture designs for building production-grade systems. 
%\subsubsection{Batch-Mode Recommendations}
1) A \textit{batch-mode recommendation architecture} refers to a pipeline where a recommender model is trained with the input data flow, and the recommendation results from that model are generated subsequently. 
In industrial recommender systems where a collaborative filtering type algorithm is used, 
this kind of architecture is highly effective. This is because the batch-mode pipeline 
hides the latency of using a model to score items and produce the top-k recommendations, 
by pre-caching the recommendation results into a database. 
In the operationalization example available in the Recommenders repository, 
11,000 request units per second are achieved in a recommendation system with minimal sized machines on cloud service. 
%To build a model with desirable performance by using an optimal budget, hyper parameter tuning techniques like Bayesian Optimization \cite{snoek2012practical} and Neural Architecture Search \cite{zoph2016neural} are often used. In the example notebooks shown in the Recommenders repository, applying an intelligent hyper parameter tuning technique can reduce time spent on parameter searching dramatically \footref{model_select}. 
To meet the compliance requirements, usually post-filtering logic is incorporated in the pipeline to refine the recommendations.
2) \textit{Real-time model serving} refers to an architecture where attributes of users and items, contextual information etc. are used during model training. In this scenario, a real-time model serving scheme is usually considered given the difficulty of pre-computing all possible recommendations for different feature and context combinations. Compared to the batch-mode recommendation, model management in this scenario is especially important because a pre-trained model is re-used for scoring and item ranking. Depending on business requirements, engineering specifications, and model complexity, computing resources for model serving should be scalable and configurable for both computational efficiency and cost saving purposes. Nowadays, Kubernetes cluster is used as the industrial main standard for model serving \cite{bernstein2014containers}. Another reference architecture that illustrates the use of LightGBM model \cite{ke2017lightgbm} for real-time model serving, built on top of the Spark framework, is demonstrated in the Azure documentation\footnote{\url{https://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/scalable-personalization}}.
