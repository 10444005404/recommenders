\subsection{Usage and Reproducibility} %NOT SURE ABOUT THE TITLE

One of our main goals in building Microsoft Recommenders has been to enhance several aspects of research and data science processes.
In particular, we provide utilities for tasks which are common and repetitive when building recommendations pipelines.  
{\em Data preparation} is the first stage of such a pipeline and involves loading data sets in the right format and generating training, validation and test sets from it.
We provide utilities which make it simple to download the Movielens \cite{movielens}, Criteo \footnote{\url{https://labs.criteo.com/2013/12/download-terabyte-click-logs/
}} and Wikidata \cite{wikidata} data sets. 
For example, to load 
%For example, the following code loads 
the Movielens 100K data set to a Pandas data frame
%\begin{minted}{python}
%from reco_utils.dataset import movielens
%data = movielens.load_pandas_df(size="100k", 
%    header=["UserId", "MovieId", "Rating", "Timestamp"], title_col="Title")
%\end{minted}
one uses the function \verb!movielens.load_pandas_df()! from the \verb!reco_utils.dataset.movielens! module.
To load the data to a PySpark data frame the syntax is similar and uses the function \verb!movielens.load_spark_df()!.

Splitting a data set can also be done in one line 
with the \verb!reco_utils.dataset.python_splitters! module.
%For example, the following code performs two types of 
For example, the function 
\verb!python_random_split()! splits by random sampling 
and the function \verb!python_chrono_split()! by splitting each user"s data according to the timestamp: 
%\begin{minted}{python}
%from reco_utils.dataset.python_splitters import python_chrono_split, 
%    python_random_split
%data_train, data_validate, data_test = python_random_split(data, 
%    ratio=[0.6, 0.2, 0.2])
%data_train, data_test = python_chrono_split(data, ratio=0.7, 
%    filter_by="user", col_user="UserId", col_item="MovieId", 
%    col_timestamp="Timestamp")
%\end{minted}
There are also methods for stratified splitting (by user or item) and chronological splitting without stratification. 
There are versions of the splitter methods for both Pandas and PySpark.

Training any of the algorithms included in the repository is also straightforward. To this end, we use an interface similar to Scikit-Learn \cite{pedregosa2011scikit} 
with algorithms defined as classes implementing \verb fit() and \verb predict() methods. For example, training a Neural Collaborative Filtering \cite{he2017neural} model
can be done as follows
\begin{minted}{python}
    model = NCF(n_users=n_users, n_items=n_items, n_factors=4, n_epochs=100,
        layer_sizes=[16,8,4], batch_size=256, learning_rate=1e-3)
    model.fit(data_train)
    preds = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]
        for (_, row) in data_test.iterrows()]
\end{minted}

Another modular part of the recommendation process is the evaluation of metrics between two data frames consisting of users, items and numerical values 
(predicted vs. ground truth). There is a distinction between rating metrics (such as mean squared error and accuracy) and ranking metrics 
(such as top-k precision and recall). For example, computing root mean squared error is done using 
the \verb!rmse()! function from the \verb!reco_utils.evaluation.python_evaluation! module, 
or the \verb!rmse()! function in the \verb!SparkRatingEvaluation! class from \verb!reco_utils.evaluation.spark_evaluation!.
%\begin{minted}{python}
%from reco_utils.evaluation.python_evaluation import rmse
%eval_rmse = rmse(data_test, predictions, col_user="UserId", 
%    col_item="MovieId", col_rating="Rating", col_prediction="Prediction")
%\end{minted}
 
These interfaces hide all the implementation details of the algorithms involved, while providing access to the necessary parameterization.
In this way, a recommendation process can be completely replicable and reproducible from beginning to end. Furthermore, any randomization 
which is part of data processing and evaluation is exposed to the user with seed parameters, thus enabling fair comparisons between the results of different algorithms
on the same data. 

