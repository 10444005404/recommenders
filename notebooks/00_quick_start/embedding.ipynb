{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/caffe/python', '/opt/caffe2/build', '/data/home/wutao/src/Recommenders/notebooks/00_quick_start', '/data/anaconda/envs/reco_base/lib/python36.zip', '/data/anaconda/envs/reco_base/lib/python3.6', '/data/anaconda/envs/reco_base/lib/python3.6/lib-dynload', '', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages/IPython/extensions', '/data/home/wutao/.ipython', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages/azureml/_project/vendor', '../..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "sys.path.append(\"../..\")  # adding Recommenders root path.\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "Pandas version: 0.24.2\n"
     ]
    }
   ],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import papermill as pm\n",
    "import gensim\n",
    "import multiprocessing\n",
    "\n",
    "from numpy import random\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.movielens import load_pandas_df\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split,python_chrono_split\n",
    "#from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, \n",
    "    precision_at_k, recall_at_k, get_top_k_items)\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:00<00:00, 9.62kKB/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_pandas_df(size='100k', genres_col='genre', title_col='title', year_col='year')\n",
    "\n",
    "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
    "data['rating'] = data['rating'].astype(np.float32)\n",
    "data[['userID','itemID']] = data[['userID','itemID']].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = python_stratified_split(data, ratio=0.75, col_user='userID', col_item='itemID', seed=42)\n",
    "train, test = python_chrono_split(data, ratio=0.75, col_user='userID', col_item='itemID')\n",
    "#train, validation, test = python_chrono_split(data, ratio=[0.7, 0.05, 0.25])\n",
    "train_dict = dict(train.groupby('userID')['itemID'].apply(list))\n",
    "test_dict = dict(test.groupby('userID')['itemID'].apply(list))\n",
    "train_users_list = list(train_dict.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distribution_list = [0] * len(train_users_list)\n",
    "\n",
    "for i in range(len(train_users_list)):\n",
    "    train_distribution_list[i] = len(train_dict[train_users_list[i]])\n",
    "\n",
    "total_viewings = sum(train_distribution_list)\n",
    "prob_train = [user_num_movies / total_viewings for user_num_movies in train_distribution_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 200000\n",
    "WINDOW = 15+1\n",
    "REP = 50\n",
    "PROPORTION_USAGE = 0.80\n",
    "ADD_UID = False\n",
    "DIM=50\n",
    "\n",
    "TOP_K = 10\n",
    "TOP_K_MULTIPLIER = 1\n",
    "\n",
    "TOPN_PERCLUSTER = 20\n",
    "#MAX_CLUSTERS = 15\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "EXP_NAME = \"chrono_200k_080_50dim_075_025\"\n",
    "datafile = EXP_NAME + \".data\"\n",
    "modelfile = EXP_NAME + \".model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO NEED TO RUN IF data file is already available\n",
    "\n",
    "with open(datafile,\"w\") as f:\n",
    "    for i in range(ITER):\n",
    "        if i / ITER < PROPORTION_USAGE:  # sample based on user's usage.  The more ratings a user has, the higher probability of being selected.\n",
    "            user_idx = random.choice(len(prob_train),p=prob_train)\n",
    "        else:  # go through user population sequentially\n",
    "            user_idx = i % len(train_users_list)\n",
    "            \n",
    "        selected_userID = train_users_list[user_idx]\n",
    "        selected_user_movies_list = train_dict[selected_userID]\n",
    "        if len(selected_user_movies_list) >= WINDOW:\n",
    "            window = WINDOW\n",
    "        else:\n",
    "            window = len(selected_user_movies_list)\n",
    "        \n",
    "        temp_movies_list = []\n",
    "        uid_string = \"u\"+str(selected_userID)\n",
    "        \n",
    "        for j in range(REP):\n",
    "# now switching to numpy.random            idxsample = random.sample(range(len(rand_user_movies_list)),window)\n",
    "            idxsample = random.randint(len(selected_user_movies_list),size=window)\n",
    "            temp_movies_list = temp_movies_list + [selected_user_movies_list[idx] for idx in idxsample]\n",
    "            if ADD_UID:\n",
    "                temp_movies_list.append(uid_string)\n",
    "                \n",
    "        f.write(' '.join(temp_movies_list)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seq():\n",
    "    with open(datafile,\"rt\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            yield line.strip().split()\n",
    "\n",
    "docs = list(read_seq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1492281758, 1598779500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = gensim.models.Word2Vec(docs, size = DIM, window = 5, min_count =2 , \n",
    "                               workers = multiprocessing.cpu_count(), sg = 1)\n",
    "model.train(docs, total_examples= len(docs), epochs = 10, compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1113.841839313507 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "train_time = time.time() - start_time\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_seen_and_uservec(predicted_list_w_scores, seen_list):\n",
    "# predicted_list_w_scores is a list of lists,[[a,b]...] where a is ther user_id and b is the similairity score\n",
    "    result_list = []\n",
    "    for a, b in predicted_list_w_scores:\n",
    "        if a not in seen_list and not a.startswith('u'):\n",
    "            result_list.append([a, b])\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def get_best_matches(userid):\n",
    "    trained_wv_list = []\n",
    "    for movie_id in train_dict[userid]:\n",
    "        trained_wv_list.append(model.wv[movie_id])\n",
    "\n",
    "    trained_wv_np = np.array(trained_wv_list)\n",
    "    rec_list = []\n",
    "\n",
    "#    for num_clusters in range(1,min(MAX_CLUSTERS,len(trained_wv_list))+1):\n",
    "#     for num_clusters in range(len(trained_wv_list),len(trained_wv_list)+1): # don't do cluster\n",
    "#         clustered_result = KMeans(n_clusters=num_clusters, random_state=0).fit(trained_wv_np)\n",
    "#         centroids = clustered_result.cluster_centers_\n",
    "    combined_rec_list = []\n",
    "    for mov_vec in trained_wv_list:\n",
    "        temp_recommended_list = model.wv.most_similar(positive = [mov_vec],topn=TOPN_PERCLUSTER)\n",
    "        temp_rec_seen_removed_list = remove_seen_and_uservec(temp_recommended_list, train_dict[userid])\n",
    "        combined_rec_list.extend(temp_rec_seen_removed_list)\n",
    "    rec_list.extend(combined_rec_list)  # this is not to pick the set of recommended movies based on a fixed clustering, but rather taking all possible clusterings and find the highest similarity\n",
    "    rec_list.sort(key = lambda element: element[1], reverse = True)  #rank based on similarity.  \n",
    "    \n",
    "# rec_list may have same movies with different scores.  Since it is sorted, in result_list we will only take the first score.\n",
    "\n",
    "    rec_movieids_list = []\n",
    "    rec_movies = 0\n",
    "    rec_list_pointer = 0\n",
    "    result_list = []\n",
    "    while rec_movies < min(TOP_K * TOP_K_MULTIPLIER,len(rec_list)): # compute a larger list as input to the classifier; don't need it if not using a classifier\n",
    "        if rec_list_pointer >= len(rec_list):\n",
    "            break\n",
    "        if rec_list[rec_list_pointer][0] not in rec_movieids_list:  #index 0 for movie id  \n",
    "            rec_movieids_list.append(rec_list[rec_list_pointer][0])  #insert movie to rec_movieids_list\n",
    "            element_list = [userid]\n",
    "            element_list.extend(rec_list[rec_list_pointer])\n",
    "            result_list.append(element_list)\n",
    "            rec_movies = rec_movies + 1\n",
    "        rec_list_pointer = rec_list_pointer + 1    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,10,100,101,102,103,104,105,106,107,108,109,11,110,111,112,113,114,115,116,117,118,119,12,120,121,122,123,124,125,126,127,128,129,13,130,131,132,133,134,135,136,137,138,139,14,140,141,142,143,144,145,146,147,148,149,15,150,151,152,153,154,155,156,157,158,159,16,160,161,162,163,164,165,166,167,168,169,17,170,171,172,173,174,175,176,177,178,179,18,180,181,182,183,184,185,186,187,188,189,19,190,191,192,193,194,195,196,197,198,199,2,20,200,201,202,203,204,205,206,207,208,209,21,210,211,212,213,214,215,216,217,218,219,22,220,221,222,223,224,225,226,227,228,229,23,230,231,232,233,234,235,236,237,238,239,24,240,241,242,243,244,245,246,247,248,249,25,250,251,252,253,254,255,256,257,258,259,26,260,261,262,263,264,265,266,267,268,269,27,270,271,272,273,274,275,276,277,278,279,28,280,281,282,283,284,285,286,287,288,289,29,290,291,292,293,294,295,296,297,298,299,3,30,300,301,302,303,304,305,306,307,308,309,31,310,311,312,313,314,315,316,317,318,319,32,320,321,322,323,324,325,326,327,328,329,33,330,331,332,333,334,335,336,337,338,339,34,340,341,342,343,344,345,346,347,348,349,35,350,351,352,353,354,355,356,357,358,359,36,360,361,362,363,364,365,366,367,368,369,37,370,371,372,373,374,375,376,377,378,379,38,380,381,382,383,384,385,386,387,388,389,39,390,391,392,393,394,395,396,397,398,399,4,40,400,401,402,403,404,405,406,407,408,409,41,410,411,412,413,414,415,416,417,418,419,42,420,421,422,423,424,425,426,427,428,429,43,430,431,432,433,434,435,436,437,438,439,44,440,441,442,443,444,445,446,447,448,449,45,450,451,452,453,454,455,456,457,458,459,46,460,461,462,463,464,465,466,467,468,469,47,470,471,472,473,474,475,476,477,478,479,48,480,481,482,483,484,485,486,487,488,489,49,490,491,492,493,494,495,496,497,498,499,5,50,500,501,502,503,504,505,506,507,508,509,51,510,511,512,513,514,515,516,517,518,519,52,520,521,522,523,524,525,526,527,528,529,53,530,531,532,533,534,535,536,537,538,539,54,540,541,542,543,544,545,546,547,548,549,55,550,551,552,553,554,555,556,557,558,559,56,560,561,562,563,564,565,566,567,568,569,57,570,571,572,573,574,575,576,577,578,579,58,580,581,582,583,584,585,586,587,588,589,59,590,591,592,593,594,595,596,597,598,599,6,60,600,601,602,603,604,605,606,607,608,609,61,610,611,612,613,614,615,616,617,618,619,62,620,621,622,623,624,625,626,627,628,629,63,630,631,632,633,634,635,636,637,638,639,64,640,641,642,643,644,645,646,647,648,649,65,650,651,652,653,654,655,656,657,658,659,66,660,661,662,663,664,665,666,667,668,669,67,670,671,672,673,674,675,676,677,678,679,68,680,681,682,683,684,685,686,687,688,689,69,690,691,692,693,694,695,696,697,698,699,7,70,700,701,702,703,704,705,706,707,708,709,71,710,711,712,713,714,715,716,717,718,719,72,720,721,722,723,724,725,726,727,728,729,73,730,731,732,733,734,735,736,737,738,739,74,740,741,742,743,744,745,746,747,748,749,75,750,751,752,753,754,755,756,757,758,759,76,760,761,762,763,764,765,766,767,768,769,77,770,771,772,773,774,775,776,777,778,779,78,780,781,782,783,784,785,786,787,788,789,79,790,791,792,793,794,795,796,797,798,799,8,80,800,801,802,803,804,805,806,807,808,809,81,810,811,812,813,814,815,816,817,818,819,82,820,821,822,823,824,825,826,827,828,829,83,830,831,832,833,834,835,836,837,838,839,84,840,841,842,843,844,845,846,847,848,849,85,850,851,852,853,854,855,856,857,858,859,86,860,861,862,863,864,865,866,867,868,869,87,870,871,872,873,874,875,876,877,878,879,88,880,881,882,883,884,885,886,887,888,889,89,890,891,892,893,894,895,896,897,898,899,9,90,900,901,902,903,904,905,906,907,908,909,91,910,911,912,913,914,915,916,917,918,919,92,920,921,922,923,924,925,926,927,928,929,93,930,931,932,933,934,935,936,937,938,939,94,940,941,942,943,95,96,97,98,99,\n",
      "Took 13.895988702774048 seconds for prediction\n"
     ]
    }
   ],
   "source": [
    "similarity_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for test_userid in test_dict:\n",
    "    print(test_userid,end=\",\")\n",
    "    similarity_list.extend(get_best_matches(test_userid))\n",
    "    \n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "print(\"\\nTook {} seconds for prediction\".format(prediction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.961808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>0.944447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>0.925983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>447</td>\n",
       "      <td>0.912425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.904091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID itemID  prediction\n",
       "0      1    588    0.961808\n",
       "1      1    228    0.944447\n",
       "2      1    436    0.925983\n",
       "3      1    447    0.912425\n",
       "4      1     29    0.904091"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similarity_df = pd.DataFrame(similarity_list,columns=['userID','itemID','prediction'])\n",
    "predict_similarity_df[['userID','itemID']] = predict_similarity_df[['userID','itemID']].astype(str)\n",
    "predict_similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_similarity_file = EXP_NAME+'_DF.csv'\n",
    "predict_similarity_df.to_csv(predict_similarity_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2 = pd.read_csv(predict_similarity_file)\n",
    "# p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding:\n",
      "MAP:\t0.045602\n",
      "NDCG:\t0.188750\n",
      "Precision@K:\t0.165960\n",
      "Recall@K:\t0.095186\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_precision = precision_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "\n",
    "print(\"Word Embedding:\")\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
