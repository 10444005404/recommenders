{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/caffe/python', '/opt/caffe2/build', '/data/home/wutao/src/Recommenders/notebooks/00_quick_start', '/data/anaconda/envs/reco_base/lib/python36.zip', '/data/anaconda/envs/reco_base/lib/python3.6', '/data/anaconda/envs/reco_base/lib/python3.6/lib-dynload', '', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages/IPython/extensions', '/data/home/wutao/.ipython', '/data/anaconda/envs/reco_base/lib/python3.6/site-packages/azureml/_project/vendor', '../..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "sys.path.append(\"../..\")  # adding Recommenders root path.\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "Pandas version: 0.24.2\n"
     ]
    }
   ],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import papermill as pm\n",
    "import gensim\n",
    "import multiprocessing\n",
    "\n",
    "from numpy import random\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.movielens import load_pandas_df\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split,python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import (\n",
    "    rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, \n",
    "    precision_at_k, recall_at_k, get_top_k_items)\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:00<00:00, 11.8kKB/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_pandas_df(size='100k', genres_col='genre', title_col='title', year_col='year')\n",
    "\n",
    "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
    "data['rating'] = data['rating'].astype(np.float32)\n",
    "data[['userID','itemID']] = data[['userID','itemID']].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = python_stratified_split(data, ratio=0.75, col_user='userID', col_item='itemID', seed=42)\n",
    "train, test = python_chrono_split(data, ratio=0.75, col_user='userID', col_item='itemID')\n",
    "#train, validation, test = python_chrono_split(data, ratio=[0.7, 0.05, 0.25])\n",
    "train_dict = dict(train.groupby('userID')['itemID'].apply(list))\n",
    "test_dict = dict(test.groupby('userID')['itemID'].apply(list))\n",
    "train_users_list = list(train_dict.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distribution_list = [0] * len(train_users_list)\n",
    "\n",
    "for i in range(len(train_users_list)):\n",
    "    train_distribution_list[i] = len(train_dict[train_users_list[i]])\n",
    "\n",
    "total_viewings = sum(train_distribution_list)\n",
    "prob_train = [user_num_movies / total_viewings for user_num_movies in train_distribution_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 50000\n",
    "WINDOW = 10+1\n",
    "REP = 50\n",
    "PROPORTION_USAGE = 0.80\n",
    "ADD_UID = False\n",
    "DIM=20\n",
    "\n",
    "TOP_K = 10\n",
    "TOP_K_MULTIPLIER = 1\n",
    "\n",
    "TOPN_PERCLUSTER = 20\n",
    "#MAX_CLUSTERS = 15\n",
    "\n",
    "random.seed(2)\n",
    "\n",
    "EXP_NAME = \"chrono_50k_080_20dim_075_025\"\n",
    "datafile = EXP_NAME + \".data\"\n",
    "modelfile = EXP_NAME + \".model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO NEED TO RUN IF data file is already available\n",
    "\n",
    "with open(datafile,\"w\") as f:\n",
    "    for i in range(ITER):\n",
    "        if i / ITER < PROPORTION_USAGE:  # sample based on user's usage.  The more ratings a user has, the higher probability of being selected.\n",
    "            user_idx = random.choice(len(prob_train),p=prob_train)\n",
    "        else:  # go through user population sequentially\n",
    "            user_idx = i % len(train_users_list)\n",
    "            \n",
    "        selected_userID = train_users_list[user_idx]\n",
    "        selected_user_movies_list = train_dict[selected_userID]\n",
    "        if len(selected_user_movies_list) >= WINDOW:\n",
    "            window = WINDOW\n",
    "        else:\n",
    "            window = len(selected_user_movies_list)\n",
    "        \n",
    "        temp_movies_list = []\n",
    "        uid_string = \"u\"+str(selected_userID)\n",
    "        \n",
    "        for j in range(REP):\n",
    "# now switching to numpy.random            idxsample = random.sample(range(len(rand_user_movies_list)),window)\n",
    "            idxsample = random.randint(len(selected_user_movies_list),size=window)\n",
    "            temp_movies_list = temp_movies_list + [selected_user_movies_list[idx] for idx in idxsample]\n",
    "            if ADD_UID:\n",
    "                temp_movies_list.append(uid_string)\n",
    "                \n",
    "        f.write(' '.join(temp_movies_list)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seq():\n",
    "    with open(datafile,\"rt\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            yield line.strip().split()\n",
    "\n",
    "docs = list(read_seq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256529348, 275000000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = gensim.models.Word2Vec(docs, size = DIM, window = 5, min_count =2 , \n",
    "                               workers = multiprocessing.cpu_count(), sg = 1)\n",
    "model.train(docs, total_examples= len(docs), epochs = 10, compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 168.14429259300232 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "train_time = time.time() - start_time\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_seen_and_uservec(predicted_list_w_scores, seen_list):\n",
    "# predicted_list_w_scores is a list of lists,[[a,b]...] where a is ther user_id and b is the similairity score\n",
    "    result_list = []\n",
    "    for a, b in predicted_list_w_scores:\n",
    "        if a not in seen_list and not a.startswith('u'):\n",
    "            result_list.append([a, b])\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def get_best_matches(userid):\n",
    "    trained_wv_list = []\n",
    "    for movie_id in train_dict[userid]:\n",
    "        trained_wv_list.append(model.wv[movie_id])\n",
    "\n",
    "    trained_wv_np = np.array(trained_wv_list)\n",
    "    rec_list = []\n",
    "    for mov_vec in trained_wv_list:\n",
    "        temp_recommended_list = model.wv.most_similar(positive = [mov_vec],topn=TOPN_PERCLUSTER)\n",
    "        temp_rec_seen_removed_list = remove_seen_and_uservec(temp_recommended_list, train_dict[userid])\n",
    "        rec_list.extend(temp_rec_seen_removed_list)\n",
    "    rec_list.sort(key = lambda element: element[1], reverse = True)  #rank based on similarity.  \n",
    "    \n",
    "# rec_list may have same movies with different scores.  Since it is sorted, in result_list we will only take the first score.\n",
    "\n",
    "    rec_movieids_list = []\n",
    "    rec_movies = 0\n",
    "    rec_list_pointer = 0\n",
    "    result_list = []\n",
    "    while rec_movies < min(TOP_K * TOP_K_MULTIPLIER,len(rec_list)): # compute a larger list as input to the classifier; don't need it if not using a classifier\n",
    "        if rec_list_pointer >= len(rec_list):\n",
    "            break\n",
    "        if rec_list[rec_list_pointer][0] not in rec_movieids_list:  #index 0 for movie id  \n",
    "            rec_movieids_list.append(rec_list[rec_list_pointer][0])  #insert movie to rec_movieids_list\n",
    "            element_list = [userid]\n",
    "            element_list.extend(rec_list[rec_list_pointer])\n",
    "            result_list.append(element_list)\n",
    "            rec_movies = rec_movies + 1\n",
    "        rec_list_pointer = rec_list_pointer + 1    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 12.106865882873535 seconds for prediction\n"
     ]
    }
   ],
   "source": [
    "similarity_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for test_userid in test_dict:\n",
    "#    print(test_userid,end=\",\")\n",
    "    similarity_list.extend(get_best_matches(test_userid))\n",
    "    \n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "print(\"\\nTook {} seconds for prediction\".format(prediction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.983964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>228</td>\n",
       "      <td>0.979110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>418</td>\n",
       "      <td>0.969349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>501</td>\n",
       "      <td>0.968286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.967822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID itemID  prediction\n",
       "0      1    588    0.983964\n",
       "1      1    228    0.979110\n",
       "2      1    418    0.969349\n",
       "3      1    501    0.968286\n",
       "4      1      5    0.967822"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similarity_df = pd.DataFrame(similarity_list,columns=['userID','itemID','prediction'])\n",
    "predict_similarity_df[['userID','itemID']] = predict_similarity_df[['userID','itemID']].astype(str)\n",
    "predict_similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_similarity_file = EXP_NAME+'_DF.csv'\n",
    "predict_similarity_df.to_csv(predict_similarity_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2 = pd.read_csv(predict_similarity_file)\n",
    "# p2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding:\n",
      "MAP:\t0.046463\n",
      "NDCG:\t0.193828\n",
      "Precision@K:\t0.171474\n",
      "Recall@K:\t0.095319\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_precision = precision_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, predict_similarity_df, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)\n",
    "\n",
    "print(\"Word Embedding:\")\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
