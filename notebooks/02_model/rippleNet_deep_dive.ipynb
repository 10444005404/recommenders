{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RippleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Pandas version: 0.25.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import argparse \n",
    "from reco_utils.evaluation.python_evaluation import auc, precision_at_k, recall_at_k\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "\n",
    "from reco_utils.recommender.ripplenet.preprocess import (read_item_index_to_entity_id_file, \n",
    "                                         convert_rating, \n",
    "                                         convert_kg)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.data_loader import (\n",
    "                                         load_kg, \n",
    "                                         get_ripple_set)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.train import (fit, predict)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.model import RippleNet\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "# Ripple parameters\n",
    "n_epoch = 10 #the number of epochs\n",
    "batch_size = 1024 #batch size\n",
    "dim = 16 #dimension of entity and relation embeddings\n",
    "n_hop = 2 #maximum hops\n",
    "kge_weight = 0.01 #weight of the KGE term\n",
    "l2_weight = 1e-7 #weight of the l2 regularization term\n",
    "lr = 0.02 #learning rate\n",
    "n_memory = 32 #size of ripple set for each hop\n",
    "item_update_mode = 'plus_transform' #how to update item at the end of each hop\n",
    "using_all_hops = True #whether using outputs of all hops or just the last hop when making prediction\n",
    "#Evaluation parameters\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems\n",
    "> Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo\n",
    "> The 27th ACM International Conference on Information and Knowledge Management (CIKM 2018)\n",
    "\n",
    "Online code of RippleNet: https://github.com/hwwang55/RippleNet\n",
    "\n",
    "To address the sparsity and cold start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve recommendation performance. This paper considers the knowledge graph as the source of side information. To address the limitations of existing embedding-based and path-based methods for knowledge-graph-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the knowledge graph into recommender systems. Similar to actual ripples propagating on the water, RippleNet stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user’s potential interests along links in the knowledge graph. The multiple \"ripples\" activated by a user’s historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item, which could be used for predicting the final clicking probability. Through extensive experiments on real-world datasets, we demonstrate that RippleNet achieves substantial gains in a variety of scenarios, including movie, book and news recommendation, over several state-of-the-art baselines.\n",
    "\n",
    "![alt text](https://github.com/hwwang55/RippleNet/raw/master/framework.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read original data and transform entity ids to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:01<00:00, 3.04kKB/s]\n"
     ]
    }
   ],
   "source": [
    "kg_original = pd.read_csv(\"https://recodatasets.blob.core.windows.net/wikidata/movielens_{}_wikidata.csv\".format(MOVIELENS_DATA_SIZE))\n",
    "ratings_original = movielens.load_pandas_df(MOVIELENS_DATA_SIZE,\n",
    "                              ('UserId', 'ItemId', 'Rating', 'Timestamp'),\n",
    "                             title_col='Title',\n",
    "                             genres_col='Genres',\n",
    "                             year_col='Year')\n",
    "rating_threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_id(df, entities_id, col_transform, col_name = \"unified_id\"):\n",
    "    df = df.merge(entities_id, left_on = col_transform, right_on = \"entity\")\n",
    "    df = df.rename(columns = {\"unified_id\": col_name})\n",
    "    return df.drop(columns = [col_transform, \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_id = \"movielens_id\"\n",
    "entities_id = pd.DataFrame({\"entity\":list(set(kg_original.original_entity)) + list(set(kg_original.linked_entities))}).reset_index()\n",
    "entities_id = entities_id.rename(columns = {\"index\": \"unified_id\"})\n",
    "\n",
    "item_to_entity = kg_original[[var_id, \"original_entity\"]].drop_duplicates().reset_index().drop(columns = \"index\")\n",
    "item_to_entity = transform_id(item_to_entity, entities_id, \"original_entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = kg_original[[\"original_entity\", \"linked_entities\"]].drop_duplicates()\n",
    "kg = transform_id(kg, entities_id, \"original_entity\", \"original_entity_id\")\n",
    "kg = transform_id(kg, entities_id, \"linked_entities\", \"linked_entities_id\")\n",
    "kg[\"relation\"] = 1\n",
    "kg_wikidata = kg[[\"original_entity_id\",\"relation\", \"linked_entities_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_movielens = [\"UserId\", \"ItemId\", \"Rating\", \"Timestamp\"]\n",
    "ratings = ratings_original[vars_movielens].sort_values(vars_movielens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess module from RippleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_index_old2new, entity_id2index = read_item_index_to_entity_id_file(item_to_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting rating file ...\n",
      "number of users: 942\n",
      "number of items: 1677\n"
     ]
    }
   ],
   "source": [
    "ratings_final = convert_rating(ratings, item_index_old2new = item_index_old2new,\n",
    "                               threshold = rating_threshold, seed = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting kg file ...\n",
      "number of entities (containing items): 22994\n",
      "number of relations: 1\n"
     ]
    }
   ],
   "source": [
    "kg_final = convert_kg(kg_wikidata, entity_id2index = entity_id2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_eval_data = python_stratified_split(ratings_final, ratio=0.6, col_user='user_index', col_item='item', seed=12)\n",
    "test_data, eval_data = python_stratified_split(test_eval_data, ratio=0.5, col_user='user_index', col_item='item', seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_dict = train_data.loc[train_data.rating == 1].groupby('user_index')['item'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "constructing ripple set ...\n"
     ]
    }
   ],
   "source": [
    "n_entity, n_relation, kg = load_kg(kg_final)\n",
    "ripple_set = get_ripple_set(kg, user_history_dict, n_hop=n_hop, n_memory=n_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple = RippleNet(dim=dim,n_hop=n_hop,\n",
    "                   kge_weight=kge_weight, l2_weight=l2_weight, lr=lr,\n",
    "                   n_memory=n_memory,\n",
    "                   item_update_mode=item_update_mode, using_all_hops=using_all_hops,\n",
    "                   n_entity=n_entity,n_relation=n_relation)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = fit(sess=sess, \n",
    "                n_epoch=n_epoch, batch_size=batch_size,n_hop=n_hop,\n",
    "                model=ripple, train_data=train_data[[\"user_index\", \"item\", \"rating\"]].to_numpy(), \n",
    "                ripple_set=ripple_set, show_loss=show_loss)\n",
    "    labels, scores = predict(sess=sess, \n",
    "                             batch_size=batch_size, n_hop=n_hop, \n",
    "                             model=model, data=test_data[[\"user_index\", \"item\", \"rating\"]].to_numpy(),\n",
    "                             ripple_set=ripple_set)\n",
    "\n",
    "predictions = [1 if i >= 0.5 else 0 for i in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = auc(test_data, test_data, \n",
    "            col_user=\"user_index\",\n",
    "            col_item=\"item\",\n",
    "            col_rating=\"rating\",\n",
    "            col_prediction=\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The auc score is {}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = np.mean(np.equal(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The acc score is {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_k_score = precision_at_k(test_data, test_data, \n",
    "            col_user=\"user_index\",\n",
    "            col_item=\"item\",\n",
    "            col_rating=\"original_rating\",\n",
    "            col_prediction=\"scores\",\n",
    "            relevancy_method=\"top_k\",\n",
    "            k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The precision_k_score score at k = {}, is {}\".format(k, precision_k_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k_score = recall_at_k(test_data, test_data, \n",
    "            col_user=\"user_index\",\n",
    "            col_item=\"item\",\n",
    "            col_rating=\"original_rating\",\n",
    "            col_prediction=\"scores\",\n",
    "            relevancy_method=\"top_k\",\n",
    "            k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall_k_score score at k = {}, is {}\".format(k, recall_k_score))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
