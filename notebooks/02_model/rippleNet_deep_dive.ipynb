{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RippleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Pandas version: 0.25.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import argparse \n",
    "from reco_utils.evaluation.python_evaluation import auc\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "\n",
    "from reco_utils.recommender.ripplenet.preprocess import (read_item_index_to_entity_id_file, \n",
    "                                         convert_rating, \n",
    "                                         convert_kg)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.data_loader import (\n",
    "                                         dataset_split,\n",
    "                                         load_kg, \n",
    "                                         get_ripple_set)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.train import (fit, predict)\n",
    "\n",
    "from reco_utils.recommender.ripplenet.model import RippleNet\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "# Ripple parameters\n",
    "n_epoch = 10 #the number of epochs\n",
    "batch_size = 1024 #batch size\n",
    "dim = 16 #dimension of entity and relation embeddings\n",
    "n_hop = 2 #maximum hops\n",
    "kge_weight = 0.01 #weight of the KGE term\n",
    "l2_weight = 1e-7 #weight of the l2 regularization term\n",
    "lr = 0.02 #learning rate\n",
    "n_memory = 32 #size of ripple set for each hop\n",
    "item_update_mode = 'plus_transform' #how to update item at the end of each hop\n",
    "using_all_hops = True #whether using outputs of all hops or just the last hop when making prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read original data and transform entity ids to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:01<00:00, 3.20kKB/s]\n"
     ]
    }
   ],
   "source": [
    "kg_original = pd.read_csv(\"https://recodatasets.blob.core.windows.net/wikidata/movielens_{}_wikidata.csv\".format(MOVIELENS_DATA_SIZE))\n",
    "ratings_original = movielens.load_pandas_df(MOVIELENS_DATA_SIZE,\n",
    "                              ('UserId', 'ItemId', 'Rating', 'Timestamp'),\n",
    "                             title_col='Title',\n",
    "                             genres_col='Genres',\n",
    "                             year_col='Year')\n",
    "rating_threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_id(df, entities_id, col_transform, col_name = \"unified_id\"):\n",
    "    df = df.merge(entities_id, left_on = col_transform, right_on = \"entity\")\n",
    "    df = df.rename(columns = {\"unified_id\": col_name})\n",
    "    return df.drop(columns = [col_transform, \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_id = \"movielens_id\"\n",
    "entities_id = pd.DataFrame({\"entity\":list(set(kg_original.original_entity)) + list(set(kg_original.linked_entities))}).reset_index()\n",
    "entities_id = entities_id.rename(columns = {\"index\": \"unified_id\"})\n",
    "\n",
    "item_to_entity = kg_original[[var_id, \"original_entity\"]].drop_duplicates().reset_index().drop(columns = \"index\")\n",
    "item_to_entity = transform_id(item_to_entity, entities_id, \"original_entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = kg_original[[\"original_entity\", \"linked_entities\"]].drop_duplicates()\n",
    "kg = transform_id(kg, entities_id, \"original_entity\", \"original_entity_id\")\n",
    "kg = transform_id(kg, entities_id, \"linked_entities\", \"linked_entities_id\")\n",
    "kg[\"relation\"] = 1\n",
    "kg_wikidata = kg[[\"original_entity_id\",\"relation\", \"linked_entities_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_movielens = [\"UserId\", \"ItemId\", \"Rating\", \"Timestamp\"]\n",
    "ratings = ratings_original[vars_movielens].sort_values(vars_movielens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess module from RippleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_index_old2new, entity_id2index = read_item_index_to_entity_id_file(item_to_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting rating file ...\n",
      "number of users: 942\n",
      "number of items: 1677\n"
     ]
    }
   ],
   "source": [
    "ratings_final = convert_rating(ratings, item_index_old2new = item_index_old2new, threshold = rating_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting kg file ...\n",
      "number of entities (containing items): 22994\n",
      "number of relations: 1\n"
     ]
    }
   ],
   "source": [
    "kg_final = convert_kg(kg_wikidata, entity_id2index = entity_id2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_eval_data = python_stratified_split(ratings_final, ratio=0.6, col_user='user_index', col_item='item', seed=12)\n",
    "test_data, eval_data = python_stratified_split(ratings_final, ratio=0.5, col_user='user_index', col_item='item', seed=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_dict = train_data.loc[train_data.rating == 1].groupby('user_index')['item'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "constructing ripple set ...\n"
     ]
    }
   ],
   "source": [
    "n_entity, n_relation, kg = load_kg(kg_final)\n",
    "ripple_set = get_ripple_set(kg, user_history_dict, n_hop=n_hop, n_memory=n_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = dict()\n",
    "for user in data[start:end, 0]:\n",
    "    feed_dict[model.memories_h[i]] = ripple_set[user][i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripple = RippleNet(dim=dim,n_hop=n_hop,\n",
    "                   kge_weight=kge_weight, l2_weight=l2_weight, lr=lr,\n",
    "                   n_memory=n_memory,\n",
    "                   item_update_mode=item_update_mode, using_all_hops=using_all_hops,\n",
    "                   n_entity=n_entity,n_relation=n_relation)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = fit(sess=sess, \n",
    "                n_epoch=n_epoch, batch_size=batch_size,n_hop=n_hop,\n",
    "                model=ripple, train_data=train_data.to_numpy(), \n",
    "                ripple_set=ripple_set, show_loss=show_loss)\n",
    "    labels, scores = predict(sess=sess, \n",
    "                             batch_size=batch_size, n_hop=n_hop, \n",
    "                             model=model, data=test_data.to_numpy(),\n",
    "                             ripple_set=ripple_set)\n",
    "\n",
    "predictions = [1 if i >= 0.5 else 0 for i in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true=labels, y_score=scores)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
